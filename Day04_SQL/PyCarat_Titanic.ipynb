{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55021e5-bd62-4210-958d-32dec0b84996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef50b081-9b20-4e00-a750-82b2639a1e9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Import necessary libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpycaret\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassification\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# --- Step 1: Load the Dataset ---\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load the Titanic dataset directly from Seaborn\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import seaborn as sns\n",
    "from pycaret.classification import *\n",
    "\n",
    "# --- Step 1: Load the Dataset ---\n",
    "# Load the Titanic dataset directly from Seaborn\n",
    "titanic_df = sns.load_dataset('titanic')\n",
    "\n",
    "# Display the first few rows to see the data\n",
    "print(\"Original Titanic Dataset:\")\n",
    "print(titanic_df.head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "# --- Step 2: Initialize PyCaret Environment ---\n",
    "# The setup() function initializes the environment and prepares the data.\n",
    "# It handles missing values, encodes categorical features, and splits the data.\n",
    "print(\"Setting up PyCaret Environment...\")\n",
    "clf_setup = setup(data=titanic_df, \n",
    "                  target='survived',        # The column we want to predict\n",
    "                  session_id=123,           # For reproducibility\n",
    "                  ignore_features=['who', 'adult_male', 'alive', 'alone'], # Redundant columns\n",
    "                  numeric_imputation='mean',\n",
    "                  categorical_imputation='most_frequent',\n",
    "                  silent=True)              # Suppresses user confirmation prompt\n",
    "print(\"Setup Complete.\\n\")\n",
    "\n",
    "\n",
    "# --- Step 3: Compare Baseline Models ---\n",
    "# This function trains and evaluates all models in the PyCaret library\n",
    "# using cross-validation. It returns a table ranking them by a chosen metric.\n",
    "print(\"Comparing all baseline models...\")\n",
    "best_model = compare_models(sort='Accuracy') # Sort by accuracy\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "# --- Step 4: Create and Tune the Best Model ---\n",
    "# Let's assume the best model from compare_models() is LightGBM ('lightgbm').\n",
    "# We can create it and then tune its hyperparameters.\n",
    "print(\"Creating a specific model (LightGBM)...\")\n",
    "lgbm = create_model('lightgbm')\n",
    "\n",
    "print(\"\\nTuning the LightGBM model...\")\n",
    "tuned_lgbm = tune_model(lgbm)\n",
    "\n",
    "\n",
    "# --- Step 5: Analyze the Tuned Model ---\n",
    "# PyCaret offers several plots to analyze model performance.\n",
    "# Here we'll plot the Confusion Matrix and the AUC curve.\n",
    "print(\"\\nPlotting model performance...\")\n",
    "# The plots will open in a new window or display inline in a notebook.\n",
    "plot_model(tuned_lgbm, plot='confusion_matrix')\n",
    "plot_model(tuned_lgbm, plot='auc')\n",
    "\n",
    "\n",
    "# --- Step 6: Finalize the Model for Deployment ---\n",
    "# Finalize trains the model on the entire dataset (including the hold-out set).\n",
    "print(\"Finalizing the tuned model...\")\n",
    "final_model = finalize_model(tuned_lgbm)\n",
    "print(\"\\nFinal Model Parameters:\")\n",
    "print(final_model)\n",
    "\n",
    "\n",
    "# --- Step 7: Make Predictions ---\n",
    "# PyCaret's setup() automatically splits the data. Let's see the unseen data.\n",
    "unseen_data = get_config('X_test')\n",
    "predictions = predict_model(final_model, data=unseen_data)\n",
    "\n",
    "print(\"\\nPredictions on unseen data:\")\n",
    "print(predictions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1fdca8-9f80-4054-ad9c-f461ccefa62f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
