{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce2d389c-7c11-45b0-9159-e4ecac18b191",
   "metadata": {},
   "source": [
    "## Collecting Data\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand different data sources.\n",
    "- Read and write data from flat files.\n",
    "- Work with APIs.\n",
    "- Scrape data from web pages.\n",
    "- Connect to SQL databases.\n",
    "- Manage large datasets efficiently.\n",
    "- Understand basic data pipelines and versioning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b4c461-7a12-4e45-9341-0d81fd3d4dee",
   "metadata": {},
   "source": [
    "![DS_Process](./images/12_page.jpg)\n",
    "\n",
    "![Collecting_Data](./images/13_page.jpg)\n",
    "\n",
    "![Collecting_Data_2](./images/14_page.jpg)\n",
    "\n",
    "![Collecting_Data_3](./images/15_page.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3448bd9e-4af6-425d-be4a-0545f731e555",
   "metadata": {},
   "source": [
    "# ðŸ“Š Where to Find Data for Data\n",
    "\n",
    "## Research & Institutional Data\n",
    "- [**NIMH Data Archive (NDA)**](https://nda.nih.gov/) â€“ Human subjects data from hundreds of research projects in mental health and neuroscience. *(requires application for access)*  \n",
    "- [**StatLib Datasets Archive**](http://lib.stat.cmu.edu/datasets/) â€“ Classic collection of older but useful datasets hosted by Carnegie Mellon.  \n",
    "- [**UCI Machine Learning Repository**](https://archive.ics.uci.edu/) â€“ Classic benchmark datasets for machine learning research.  \n",
    "\n",
    "---\n",
    "\n",
    "## Competitions & Community\n",
    "- [**Kaggle Datasets**](https://www.kaggle.com/datasets) â€“ Thousands of community-contributed datasets, with notebooks and competitions.  \n",
    "- [**r/datasets (Reddit)**](https://www.reddit.com/r/datasets/) â€“ A subreddit where users post and request datasets.  \n",
    "- [**Awesome Public Datasets (GitHub)**](https://github.com/awesomedata/awesome-public-datasets) â€“ Curated list of dataset sources across domains.  \n",
    "\n",
    "---\n",
    "\n",
    "## Open Data Portals\n",
    "- [**Google Dataset Search**](https://datasetsearch.research.google.com/) â€“ Like Google search, but just for datasets.  \n",
    "- [**Data.gov**](https://www.data.gov/) â€“ US Government open data (health, climate, education, etc.).  \n",
    "- [**European Union Open Data Portal**](https://data.europa.eu/) â€“ EU datasets across policy areas.  \n",
    "- [**UN Data**](http://data.un.org/) â€“ Global statistics from the United Nations.  \n",
    "- [**World Bank Open Data**](https://data.worldbank.org/) â€“ Economic, social, and development indicators.  \n",
    "\n",
    "---\n",
    "\n",
    "## APIs & Crawls\n",
    "- [**Programmable Web**](https://www.programmableweb.com) â€“ Large directory of public APIs (some links are aging, but still useful for discovery).  \n",
    "- [**Common Crawl**](https://commoncrawl.org/) â€“ Open web crawl data for text mining, NLP, and search projects.  \n",
    "- [**Internet Archive**](https://archive.org/) â€“ Huge archive of books, websites, audio, video, and more.  \n",
    "- **Other APIs**: Twitter/X API, Reddit API, News API, Spotify API â€“ Great for building projects around social or media data.  \n",
    "\n",
    "---\n",
    "\n",
    "## Special Interest Data Sources\n",
    "- [**IMDb Datasets**](https://www.imdb.com/interfaces/) â€“ Movie/TV metadata.  \n",
    "- [**OpenStreetMap (OSM)**](https://www.openstreetmap.org/) â€“ Global geospatial data (with [Overpass API](https://overpass-api.de/)).  \n",
    "- [**FiveThirtyEight Data**](https://data.fivethirtyeight.com/) â€“ Datasets behind 538â€™s journalism.  \n",
    "- [**Our World in Data**](https://ourworldindata.org/) â€“ Research-driven datasets on health, climate, energy, etc.  \n",
    "- [**Awesome ML Datasets (Wikipedia list)**](https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research) â€“ Overview of standard datasets.  \n",
    "\n",
    "---\n",
    "\n",
    "## Personal Data Projects\n",
    "- **Export Your Own Data**: Google Takeout, Facebook/Instagram export, Fitbit/Apple Health, bank statements.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66026337",
   "metadata": {},
   "source": [
    "## Data Acquisition\n",
    "\n",
    "Data acquisition is the process of collecting and preparing data from various sources. It's often the most time-consuming part of a data science project. Without high-quality data, analysis and modeling will be flawed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e559a9ad",
   "metadata": {},
   "source": [
    "## Reading Flat Files\n",
    "\n",
    "We start by reading common file types such as CSV, Excel, JSON, and Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d75e2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV\n",
    "# df_csv = pd.read_csv('data.csv')\n",
    "\n",
    "# Excel\n",
    "# df_excel = pd.read_excel('data.xlsx')\n",
    "\n",
    "# JSON\n",
    "# df_json = pd.read_json('data.json')\n",
    "\n",
    "# Parquet\n",
    "# df_parquet = pd.read_parquet('data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed056ad5",
   "metadata": {},
   "source": [
    "## Accessing APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9ef316-93af-4944-b3da-7bf089191e70",
   "metadata": {},
   "source": [
    "APIs (Application Programming Interfaces) allow us to **communicate with other applications or services** over the internet.  \n",
    "One of the most common types of APIs is the **REST API** (Representational State Transfer).\n",
    "\n",
    "## ðŸ”¹ What is a REST API?\n",
    "- A REST API exposes **endpoints** (URLs) that clients can send requests to.  \n",
    "- Clients typically use the **HTTP methods**:\n",
    "  - `GET` â†’ retrieve data  \n",
    "  - `POST` â†’ send new data  \n",
    "  - `PUT/PATCH` â†’ update data  \n",
    "  - `DELETE` â†’ remove data  \n",
    "- The server responds with data, often in **JSON format**, which is easy to parse in Python.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "953481a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random cat fact: The claws on the catâ€™s back paws arenâ€™t as sharp as the claws on the front paws because the claws in the back donâ€™t retract and, consequently, become worn.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## ðŸ”¹ Example 1: Using the `requests` library\n",
    "import requests\n",
    "\n",
    "# API endpoint (a RESTful URL)\n",
    "url = 'https://catfact.ninja/fact'\n",
    "\n",
    "# Send a GET request to the API\n",
    "response = requests.get(url)\n",
    "\n",
    "# Convert the response (JSON) into a Python dictionary\n",
    "data = response.json()\n",
    "\n",
    "# Print the result\n",
    "print(\"Random cat fact:\", data['fact'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19ff92dd-4065-42df-bd8e-2d16445017f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random dog image URL: https://images.dog.ceo/breeds/schnauzer-miniature/n02097047_151.jpg\n"
     ]
    }
   ],
   "source": [
    "#example 2\n",
    "import requests\n",
    "\n",
    "url = 'https://dog.ceo/api/breeds/image/random'\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "print(\"Random dog image URL:\", data['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2665fa",
   "metadata": {},
   "source": [
    "## Web Scraping\n",
    "\n",
    "Use BeautifulSoup to extract data from web pages. Use responsibly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17775918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A Light in the Attic',\n",
       " 'Tipping the Velvet',\n",
       " 'Soumission',\n",
       " 'Sharp Objects',\n",
       " 'Sapiens: A Brief History of Humankind']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = requests.get('https://books.toscrape.com/').text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "titles = [book.h3.a['title'] for book in soup.select('.product_pod')]\n",
    "titles[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6b4e33",
   "metadata": {},
   "source": [
    "## SQL Databases\n",
    "\n",
    "Use SQLite for local testing. In production, you'd use PostgreSQL, MySQL, or similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c3b1655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [name]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('example.db')\n",
    "df_sql = pd.read_sql_query('SELECT name FROM sqlite_master WHERE type=\"table\";', conn)\n",
    "df_sql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eefa92f",
   "metadata": {},
   "source": [
    "## Large Dataset Handling\n",
    "\n",
    "Read data in chunks or optimize data types to handle large files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf82df45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average temperature: 18.88916827852998\n"
     ]
    }
   ],
   "source": [
    "# for chunk in pd.read_csv('./data/forestfires.csv', chunksize=10000):\n",
    "#     process(chunk)\n",
    "\n",
    "\n",
    "total_temp = 0\n",
    "total_rows = 0\n",
    "\n",
    "for chunk in pd.read_csv('./data/forestfires.csv', chunksize=1000):\n",
    "    total_temp += chunk['temp'].sum()\n",
    "    total_rows += len(chunk)\n",
    "\n",
    "average_temp = total_temp / total_rows\n",
    "print(\"Average temperature:\", average_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa478df5-c87a-49f8-b351-b4b696d8541b",
   "metadata": {},
   "source": [
    "# ðŸ”„ Data Pipelines\n",
    "\n",
    "## ðŸ”¹ What is a Data Pipeline?\n",
    "A **data pipeline** is a series of steps that data goes through from its **source** to its **destination**.  \n",
    "It is designed to **move, transform, and deliver** data so that it becomes useful for analysis, reporting, or applications.\n",
    "\n",
    "\n",
    "## ðŸ”¹ Typical Stages of a Data Pipeline\n",
    "1. **Ingestion** â€“ Collecting raw data from sources such as databases, sensors, files, APIs, or logs.  \n",
    "2. **Transformation** â€“ Cleaning, filtering, normalizing, aggregating, or enriching the data.  \n",
    "3. **Storage** â€“ Saving data in a database, data warehouse, data lake, or other storage system.  \n",
    "4. **Analysis / Consumption** â€“ Using the data for reporting, dashboards, machine learning, or business applications.  \n",
    "\n",
    "\n",
    "\n",
    "## ðŸ”¹ Example (Conceptual)\n",
    "Imagine an e-commerce company:\n",
    "- **Ingestion**: Sales transactions are captured from the website.  \n",
    "- **Transformation**: Data is cleaned (e.g., fixing missing values, converting currencies).  \n",
    "- **Storage**: Processed data is stored in a data warehouse.  \n",
    "- **Analysis**: Analysts and data scientists build dashboards to track revenue trends.  \n",
    "\n",
    "\n",
    "## ðŸ”¹ Why Data Pipelines Are Important\n",
    "- **Automation**: Reduces manual work by ensuring data flows automatically.  \n",
    "- **Scalability**: Handles large and growing volumes of data.  \n",
    "- **Consistency**: Applies the same transformations each time.  \n",
    "- **Reproducibility**: Results can be trusted because the steps are standardized.  \n",
    "- **Collaboration**: Shared data flows make it easier for teams to work with the same data.  \n",
    "\n",
    "\n",
    "## ðŸ”¹ Tools Commonly Used\n",
    "- **Batch Pipelines**: Apache Airflow, Luigi, Prefect.  \n",
    "- **Streaming Pipelines**: Apache Kafka, Apache Spark Streaming, Flink.  \n",
    "- **Cloud Pipelines**: AWS Glue, Google Dataflow, Azure Data Factory.  \n",
    "\n",
    "\n",
    "ðŸ‘‰ In summary: A data pipeline is like an **assembly line for data** â€” raw material goes in, gets processed, and comes out ready for use.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcf8958-b5f4-400a-a8bf-1b59fb903493",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
